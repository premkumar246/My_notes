<h1 style='font-family:consolas;font-size:30px'> Deep Learning & Neural Networks </h1>

<ul style='font-family:calibri;font-size:17px'>
<li>This page gives you a detailed guide to kisck start your journey to Deep learnig and also acts as a foundation for Artificail Intelligence. The below guide starts with an introduction to deep learning and neural networks and provides a list of topics and resources to explore & learn in detail.</li>
</ul>

<h2 style='font-family:consolas;font-size:25px'>Deep Learning</h2>

<ul style='font-family:calibri;font-size:17px'>
<li> Deep learning is a subfield of machine learning that focuses on the use of artificial neural networks to model and solve complex problems. </li>
<li>Neural networks are computational models inspired by the structure and function of the human brain. </li>
<li>Deep learning, in particular, refers to the use of deep neural networks, which are characterized by having multiple layers (hence "deep") between the input and output layers.</li>
<li>Deep learning extends the capabilities of traditional machine learning algorithms by leveraging deep neural networks. </li>
<li>These networks can automatically learn hierarchical representations of data, allowing them to discover intricate patterns and features in large and complex datasets. </li>
<li>The depth of the network enables it to learn increasingly abstract and complex features at each layer.</li>
</ul>

<h2 style='font-family:consolas;font-size:25px'>Neural Networks</h2>
<ul style='font-family:calibri;font-size:17px'>
<li>At the core of deep learning are neural networks. </li>
<li>These networks are composed of interconnected nodes, called neurons, organized into layers. </li>
<li>The typical architecture consists of an input layer, one or more hidden layers, and an output layer. </li>
<li>Each connection between neurons has an associated weight, and each neuron applies an activation function to its weighted inputs.</li>

<li>The learning in neural networks occurs through a process called training. </li>
<li>During training, the network is exposed to a dataset with input-output pairs. </li>
<li>The weights are adjusted iteratively based on the error or the difference between the predicted outputs and the actual outputs. </li>
<li>This process involves backpropagation, where the error is propagated backward through the network, and gradient descent is used to update the weights.</li>
<li> Deep learning has revolutionized many fields by achieving state-of-the-art performance in tasks that were once considered challenging. </li>
<li> Deep learning finds applications in various domains, including image and speech recognition, natural language processing, computer vision, healthcare, finance, and more.</li>
<li> The ability to automatically learn hierarchical features makes deep learning models powerful tools for handling complex and unstructured data.</li>
<li> Here is the list of topics which this reasource cover in details step by step</li> 

</ul>

<h2 style='font-family:consolas;font-size:25px'>Topics List</h2>

<ol style='font-family:calibri;font-size:20px'>
<li>Introduction to Deep Leanring </li>
<li>Artificial Neural Networks</li>
<ol>
<li> Perceptron </li>
<li> Multi-layer Perceptron </li>
   <ol>
   <li> Multi-layer Perceptron Working </li>
   <li> Loss Functions </li>
   <li> Forward Propagation </li>
   <li> Loss Functions </li>
   <li> Backward Propagation </li>
   </ol>

<li> Performance Improvement </li>

   <ol>
   <li> Early Stopping </li>
   <li> Data Scalling </li>
   <li> Dropout Layers </li>
   <li> Regularization Techniuqes </li>
   <li> Activation Functions</li>
   <li> Activation Functions</li>
   <li> Weight Initialization Techniques</li>
   <li> Bacth Normalization </li>
   <li> Optimizers </li>
   <li> Hyperparameter Tuning </li>
   </ol>


<>Introduction to Deep Learning
1. What is Deep Learning?, Deep Learning vs Machine Learning
2. Types or Neural Networks | History of Deep Learning | Applications of DL

### Artificial Neural Networks

#### Perceptron
3. What is a Perceptron? | Perceptron vs Neuron | Geometric Intution of Perceptron - 39:00
4. Training a Perceptron
5. Perceptron Loss function | Hinge Loss | Binary Cross Entropy | Sigmoid Function
6. Problem with Perceptron
#### Multi Layer Perceptron
7. MLP Notation 
8. Multi Layer Perceptron
##### MLP Learning 
9. Forward Propagation | How does a MLP predicts output?
10. Customer churn prediction 
11. Hand written digit classification 
12. Graduate Admission Prediction
##### Loss Function
13. Loss function in Deep Learning 
##### Learning
14. Back Propagation in DL 
15. Back Propagation | The How?
16. Back Propagation | The why?
17. MLP Memoization
18. Gradient Descent in Neural Networks
19. Vanishing Gradient Problem in ANN
#### Performance Improvement
20. How to improve the performance of a Neural Network 
##### Early Stopping
21. Early stopping in Neural Networks - 12:00 
##### Data Scalling
22. Data scalling in Neural Networks - 16:55 
##### Dropout layers
23. Dropout layer in Deep learning - 27:51
24. Dropout layer in ANN - 19:17 
##### Regularization
25. Regularization in Deep learning | L2 Regularization in ANN | L1 Regularization | weight decay in ANN - 35:57 
##### Activation functions
26. Activation functions in Deep learning | Sigmoid, Tanh and Relu - 44:52 
27. Relu variants Explained | Leaky Relu | Parametric Relu | Elu | Selu - 33:25 
##### Weight Initialization Techniques
28. Weight initialization Techniques - 49:24 
29. Xavier/Glorat and He Weight Initialization - 21:07 
##### Bacth Normalization 
30. Batch Normalization - 43:39 
##### Optimizers 
31. Optimizers in Deep Learning - 22:34 
32. Exponential weighted moving averages - 18:51 
33. SGD with Momentum - 38:25 
34. Nestov Accelerated Gradient - 27:49 
35. AdaGrad Explaimed - 26:29 
36. RMS Prop - 12:38 
37. Adam Optimizer - 12:39 
##### Hyperparameter Tuning 
38. Keras Tuner | Hyperparameter tuning - 1:05:34